#### LLM101
(new) KV Cache, RadixAttention (ref [SGLang Arch | HF](https://huggingface.co/blog/paresh2806/sglang-efficient-llm-workflows#1-smarter-memory-management-with-radixattention))

---
#### 09-09-25  | Friday
##### [SGLang architecture | HF](https://huggingface.co/blog/paresh2806/sglang-efficient-llm-workflows)
* SGL uses "RadixAttention" in combination with KV Cache to speed up inference, what is RadixAtt, and how it's used in SGL
* SGL uses "Compressed Finite State Machine (FSM)" to get structured output, this is diff than constrained decoding (CD is bad when working with code). Hence, what's C-FSM and how is it diff from CD
###### Thinking and . . .
* [Why We Think | Lil'Log](https://lilianweng.github.io/posts/2025-05-01-thinking/)
* [we think too much and feel too little | r/Showerthoughts](https://www.reddit.com/r/Showerthoughts/comments/5l9l0z/charlie_chaplin_once_said_we_think_too_much_and/)
* [Do we all need a little time simply to sit and think? | Aeon Essays](https://aeon.co/essays/do-we-all-need-a-little-time-simply-to-sit-and-think)
#### 29-08-25  | Friday
* #later [Building your own CLI Coding Agent with Pydantic-AI](https://martinfowler.com/articles/build-own-coding-agent.html)
* [The Most Important Machine Learning Equations: A Comprehensive Guide](https://chizkidd.github.io//2025/05/30/machine-learning-key-math-eqns/)
* 
