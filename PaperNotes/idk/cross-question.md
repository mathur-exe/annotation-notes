#### Transformers
Q] *In transformer architecture, there are two main component: attention mechanism and multi-layer Perceptron. What’s the purpose of each of the layers?*


Q] *Having multi-head helps the model learn pattern but how is it made sure there are no redundancies, i.e. they are learning separate features*


Q] *In MLP the dimension of matrix from attention is subject to linear transformation through GELU activation where the dimension is first increase and decreases. What’s the point of this dimension change?*
